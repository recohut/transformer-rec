{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sparsh-ai/reco-book/blob/stage/nbs/bert4rec_movielens_25m_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlNI72qJZnUv"
      },
      "source": [
        "# BERT4Rec on ML-25M\n",
        "\n",
        "**Description:** Implementing BERT4Rec model on Movielens-25m dataset in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OZVcq6rYphR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPJ9X5rjXzwI"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "mkdir /content/_temp\n",
        "cd /content/_temp\n",
        "wget https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
        "unzip ml-25m.zip\n",
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqn_uGP6YuEe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 924 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 34.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 123 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 282 kB 39.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 37.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 294 kB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 57.2 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pytorch_lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehDGA7GYse2"
      },
      "source": [
        "## Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293b8dieYSoR"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    PAD = 0\n",
        "    MASK = 1\n",
        "    CAP = 0\n",
        "    SEED = 42\n",
        "    RAW_DATA_PATH = '/content/_temp/ml-25m'\n",
        "    VOCAB_SIZE = 10000\n",
        "    CHANNELS = 128\n",
        "    DROPOUT = 0.4\n",
        "    LR = 1e-4\n",
        "    HISTORY_SIZE = 120\n",
        "    DEBUG_MODE = True\n",
        "    DEBUG_LOAD = 1000\n",
        "    LOG_DIR = '/content/recommender_logs'\n",
        "    MODEL_DIR = '/content/recommender_models'\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 2000\n",
        "\n",
        "args = Args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM0d1bGcYhRV"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7duH6OQ1YzZ4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy459Dd2Yz0d"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ila65IcJaBf"
      },
      "outputs": [],
      "source": [
        "def map_column(df: pd.DataFrame, col_name: str):\n",
        "    \"\"\"Maps column values to integers.\n",
        "    \"\"\"\n",
        "    values = sorted(list(df[col_name].unique()))\n",
        "    mapping = {k: i + 2 for i, k in enumerate(values)}\n",
        "    inverse_mapping = {v: k for k, v in mapping.items()}\n",
        "    df[col_name + \"_mapped\"] = df[col_name].map(mapping)\n",
        "    return df, mapping, inverse_mapping\n",
        "\n",
        "def get_context(df: pd.DataFrame, split: str, context_size: int = 120, val_context_size: int = 5, seed: int = 42):\n",
        "    \"\"\"Create a training / validation samples.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    if split == \"train\":\n",
        "        end_index = random.randint(10, df.shape[0] - val_context_size)\n",
        "    elif split in [\"val\", \"test\"]:\n",
        "        end_index = df.shape[0]\n",
        "    else:\n",
        "        raise ValueError\n",
        "    start_index = max(0, end_index - context_size)\n",
        "    context = df[start_index:end_index]\n",
        "    return context\n",
        "\n",
        "def pad_arr(arr: np.ndarray, expected_size: int = 30):\n",
        "    \"\"\"Pad top of array when there is not enough history.\n",
        "    \"\"\"\n",
        "    arr = np.pad(arr, [(expected_size - arr.shape[0], 0), (0, 0)], mode=\"edge\")\n",
        "    return arr\n",
        "\n",
        "def pad_list(list_integers, history_size: int, pad_val: int = 0, mode=\"left\"):\n",
        "    \"\"\"Pad list from left or right\n",
        "    \"\"\"\n",
        "    if len(list_integers) < history_size:\n",
        "        if mode == \"left\":\n",
        "            list_integers = [pad_val] * (history_size - len(list_integers)) + list_integers\n",
        "        else:\n",
        "            list_integers = list_integers + [pad_val] * (history_size - len(list_integers))\n",
        "    return list_integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAXCIhFBTQsN"
      },
      "outputs": [],
      "source": [
        "def masked_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor, mask: torch.Tensor):\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "    y_true = torch.masked_select(y_true, mask)\n",
        "    predicted = torch.masked_select(predicted, mask)\n",
        "    acc = (y_true == predicted).double().mean()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do_cLe-4YgRM"
      },
      "outputs": [],
      "source": [
        "def masked_ce(y_pred, y_true, mask):\n",
        "    loss = F.cross_entropy(y_pred, y_true, reduction=\"none\")\n",
        "    loss = loss * mask\n",
        "    return loss.sum() / (mask.sum() + 1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmX_49DMlumu"
      },
      "outputs": [],
      "source": [
        "def mask_list(l1, p=0.8):\n",
        "    random.seed(args.SEED)\n",
        "    l1 = [a if random.random() < p else args.MASK for a in l1]\n",
        "    return l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVIfejNVT_a0"
      },
      "outputs": [],
      "source": [
        "def mask_last_elements_list(l1, val_context_size: int = 5):\n",
        "    l1 = l1[:-val_context_size] + mask_list(l1[-val_context_size:], p=0.5)\n",
        "    return l1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-usSJNrHY66A"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKFEK03VZNTZ"
      },
      "outputs": [],
      "source": [
        "class ML25Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, args, split='train'):\n",
        "        self.args = args\n",
        "        self.grp_by = None\n",
        "        self.groups = None\n",
        "        self.split = split\n",
        "        self.history_size = args.HISTORY_SIZE\n",
        "        self.mapping = None\n",
        "        self.inverse_mapping = None\n",
        "        self.load_dataset()\n",
        "\n",
        "    def load_dataset(self):\n",
        "        filepath = os.path.join(self.args.RAW_DATA_PATH, 'ratings.csv')\n",
        "        if args.DEBUG_MODE:\n",
        "            data = pd.read_csv(filepath, nrows=1000)\n",
        "        else:\n",
        "            data = pd.read_csv(filepath)\n",
        "        data.sort_values(by=\"timestamp\", inplace=True)\n",
        "        data, self.mapping, self.inverse_mapping = map_column(data, col_name=\"movieId\")\n",
        "        self.grp_by = data.groupby(by=\"userId\")\n",
        "        self.groups = list(self.grp_by.groups)\n",
        "        \n",
        "    def genome_mapping(self, genome):\n",
        "        \"\"\"movie id to relevance mapping\n",
        "        \"\"\"\n",
        "        genome.sort_values(by=[\"movieId\", \"tagId\"], inplace=True)\n",
        "        movie_genome = genome.groupby(\"movieId\")[\"relevance\"].agg(list).reset_index()\n",
        "        movie_genome = {a: b for a, b in zip(movie_genome['movieId'], movie_genome['relevance'])}\n",
        "        return movie_genome\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.groups)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        group = self.groups[idx]\n",
        "        df = self.grp_by.get_group(group)\n",
        "        context = get_context(df, split=self.split, context_size=self.history_size)\n",
        "        trg_items = context[\"movieId_mapped\"].tolist()\n",
        "        if self.split == \"train\":\n",
        "            src_items = mask_list(trg_items)\n",
        "        else:\n",
        "            src_items = mask_last_elements_list(trg_items)\n",
        "        pad_mode = \"left\" if random.random() < 0.5 else \"right\"\n",
        "        trg_items = pad_list(trg_items, history_size=self.history_size, mode=pad_mode)\n",
        "        src_items = pad_list(src_items, history_size=self.history_size, mode=pad_mode)\n",
        "        src_items = torch.tensor(src_items, dtype=torch.long)\n",
        "        trg_items = torch.tensor(trg_items, dtype=torch.long)\n",
        "        return src_items, trg_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2mxEHOOZGWr"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E0d-NsuZPTJ"
      },
      "outputs": [],
      "source": [
        "class BERT4Rec(pl.LightningModule):\n",
        "    def __init__(self, args, vocab_size):\n",
        "        super().__init__()\n",
        "        self.cap = args.CAP\n",
        "        self.mask = args.MASK\n",
        "        self.lr = args.LR\n",
        "        self.dropout = args.DROPOUT\n",
        "        self.vocab_size = vocab_size\n",
        "        self.channels = args.CHANNELS\n",
        "\n",
        "        self.item_embeddings = torch.nn.Embedding(\n",
        "            self.vocab_size, embedding_dim=self.channels\n",
        "        )\n",
        "        self.input_pos_embedding = torch.nn.Embedding(512, embedding_dim=self.channels)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.channels, nhead=4, dropout=self.dropout\n",
        "        )\n",
        "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "        self.linear_out = Linear(self.channels, self.vocab_size)\n",
        "        self.do = nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def encode_src(self, src_items):\n",
        "        src_items = self.item_embeddings(src_items)\n",
        "        batch_size, in_sequence_len = src_items.size(0), src_items.size(1)\n",
        "        pos_encoder = (\n",
        "            torch.arange(0, in_sequence_len, device=src_items.device)\n",
        "            .unsqueeze(0)\n",
        "            .repeat(batch_size, 1)\n",
        "        )\n",
        "        pos_encoder = self.input_pos_embedding(pos_encoder)\n",
        "        src_items += pos_encoder\n",
        "        src = src_items.permute(1, 0, 2)\n",
        "        src = self.encoder(src)\n",
        "        return src.permute(1, 0, 2)\n",
        "\n",
        "    def forward(self, src_items):\n",
        "        src = self.encode_src(src_items)\n",
        "        out = self.linear_out(src)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        src_items, y_true = batch\n",
        "        y_pred = self(src_items)\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_accuracy\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src_items, y_true = batch\n",
        "        y_pred = self(src_items)\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        self.log(\"valid_loss\", loss)\n",
        "        self.log(\"valid_accuracy\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        src_items, y_true = batch\n",
        "        y_pred = self(src_items)\n",
        "        y_pred = y_pred.view(-1, y_pred.size(2))\n",
        "        y_true = y_true.view(-1)\n",
        "        src_items = src_items.view(-1)\n",
        "        mask = src_items == self.mask\n",
        "        loss = masked_ce(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        accuracy = masked_accuracy(y_pred=y_pred, y_true=y_true, mask=mask)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        self.log(\"test_accuracy\", accuracy)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=10, factor=0.1\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": scheduler,\n",
        "            \"monitor\": \"valid_loss\",\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju9vFxseZHCK"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJyXo8psZSIz"
      },
      "outputs": [],
      "source": [
        "args.DEBUG_MODE = True\n",
        "args.DEBUG_LOAD = 10000\n",
        "train_data = ML25Dataset(args, split='train')\n",
        "val_data = ML25Dataset(args, split='val')\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=args.BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    shuffle=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_data,\n",
        "    batch_size=args.BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "model = BERT4Rec(\n",
        "    args, vocab_size=len(train_data.mapping) + 2)\n",
        "\n",
        "logger = TensorBoardLogger(\n",
        "    save_dir=args.LOG_DIR,\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor=\"valid_loss\",\n",
        "    mode=\"min\",\n",
        "    dirpath=args.MODEL_DIR,\n",
        "    filename=\"recommender\",\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=args.EPOCHS,\n",
        "    gpus=1,\n",
        "    logger=logger,\n",
        "    callbacks=[checkpoint_callback],\n",
        ")\n",
        "trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "result_val = trainer.test(test_dataloaders=val_loader)\n",
        "\n",
        "output_json = {\n",
        "    \"val_loss\": result_val[0][\"test_loss\"],\n",
        "    \"best_model_path\": checkpoint_callback.best_model_path,\n",
        "}\n",
        "\n",
        "print(output_json)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7u3_SyPZHrn"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_kL-604D95Y"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   movieId  ...                                       genres\n",
              "0        1  ...  Adventure|Animation|Children|Comedy|Fantasy\n",
              "1        2  ...                   Adventure|Children|Fantasy\n",
              "2        3  ...                               Comedy|Romance\n",
              "3        4  ...                         Comedy|Drama|Romance\n",
              "4        5  ...                                       Comedy\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_path = \"/content/ml-25m/movies.csv\"\n",
        "movies = pd.read_csv(movies_path)\n",
        "movies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwxX14ivC3Fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 4]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args.DEBUG_MODE = True\n",
        "args.DEBUG_LOAD = 10000\n",
        "data = ML25Dataset(args, split='train')\n",
        "\n",
        "random.seed(args.SEED)\n",
        "random.sample(list(data.grp_by.groups), k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOAsjdSSEqDY"
      },
      "outputs": [],
      "source": [
        "model = BERT4Rec(args, vocab_size=len(data.mapping) + 2)\n",
        "model.eval()\n",
        "\n",
        "model_path = \"/content/recommender_models/recommender.ckpt\"\n",
        "model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n",
        "movie_to_idx = {a: data.mapping[b] for a, b in zip(movies.title.tolist(), movies.movieId.tolist()) if b in data.mapping}\n",
        "idx_to_movie = {v: k for k, v in movie_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmc36T7q5OpM"
      },
      "outputs": [],
      "source": [
        "def predict(list_movies, model, movie_to_idx, idx_to_movie):\n",
        "    ids = [args.PAD] * (120 - len(list_movies) - 1) + [movie_to_idx[a] for a in list_movies] + [args.MASK]\n",
        "    src = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        prediction = model(src)\n",
        "    masked_pred = prediction[0, -1].numpy()\n",
        "    sorted_predicted_ids = np.argsort(masked_pred).tolist()[::-1]\n",
        "    sorted_predicted_ids = [a for a in sorted_predicted_ids if a not in ids]\n",
        "    return [idx_to_movie[a] for a in sorted_predicted_ids[:30] if a in idx_to_movie]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLzE_aWE6JTQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Pirates of the Caribbean: The Curse of the Black Pearl (2003)',\n",
              " 'Ratatouille (2007)',\n",
              " 'Pulp Fiction (1994)',\n",
              " 'Evolution (2001)',\n",
              " 'Jupiter Ascending (2015)',\n",
              " 'Kung Fu Panda 3 (2016)',\n",
              " 'Back to the Future (1985)',\n",
              " 'Neighbors (2014)',\n",
              " 'Bridge of Spies (2015)',\n",
              " \"Schindler's List (1993)\",\n",
              " 'Coraline (2009)',\n",
              " 'Spider-Man (2002)',\n",
              " 'X-Men (2000)',\n",
              " 'Watership Down (1978)',\n",
              " 'In the Mood For Love (Fa yeung nin wa) (2000)',\n",
              " 'Burn After Reading (2008)',\n",
              " 'Aladdin (1992)',\n",
              " 'Finding Nemo (2003)',\n",
              " 'RocknRolla (2008)',\n",
              " 'Last Castle, The (2001)',\n",
              " \"Before the Devil Knows You're Dead (2007)\",\n",
              " 'Through a Glass Darkly (Såsom i en spegel) (1961)',\n",
              " 'Looper (2012)',\n",
              " 'Hoosiers (a.k.a. Best Shot) (1986)',\n",
              " 'Kill Bill: Vol. 2 (2004)',\n",
              " 'Big Fish (2003)',\n",
              " 'Star Wars: Episode III - Revenge of the Sith (2005)',\n",
              " 'Clear and Present Danger (1994)',\n",
              " 'Flightplan (2005)',\n",
              " 'Wedding Crashers (2005)']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_movies = [\"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\",\n",
        "               \"Harry Potter and the Chamber of Secrets (2002)\",\n",
        "               \"Harry Potter and the Prisoner of Azkaban (2004)\",\n",
        "               \"Harry Potter and the Goblet of Fire (2005)\"]\n",
        "\n",
        "top_movie = predict(list_movies, model, movie_to_idx, idx_to_movie)\n",
        "top_movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbSlRaGH6JQm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Pirates of the Caribbean: The Curse of the Black Pearl (2003)',\n",
              " 'Ratatouille (2007)',\n",
              " 'Evolution (2001)',\n",
              " 'Pulp Fiction (1994)',\n",
              " 'Jupiter Ascending (2015)',\n",
              " 'Kung Fu Panda 3 (2016)',\n",
              " 'Neighbors (2014)',\n",
              " 'Back to the Future (1985)',\n",
              " 'In the Mood For Love (Fa yeung nin wa) (2000)',\n",
              " 'Bridge of Spies (2015)',\n",
              " \"Schindler's List (1993)\",\n",
              " 'Coraline (2009)',\n",
              " 'X-Men (2000)',\n",
              " 'Watership Down (1978)',\n",
              " 'Burn After Reading (2008)',\n",
              " 'RocknRolla (2008)',\n",
              " \"Before the Devil Knows You're Dead (2007)\",\n",
              " 'Aladdin (1992)',\n",
              " 'Last Castle, The (2001)',\n",
              " 'Finding Nemo (2003)',\n",
              " 'Wedding Crashers (2005)',\n",
              " 'Through a Glass Darkly (Såsom i en spegel) (1961)',\n",
              " 'Flightplan (2005)',\n",
              " 'Looper (2012)',\n",
              " 'Star Wars: Episode III - Revenge of the Sith (2005)',\n",
              " '2046 (2004)',\n",
              " 'Kill Bill: Vol. 2 (2004)',\n",
              " 'Clear and Present Danger (1994)',\n",
              " 'Port of Shadows (Quai des brumes) (1938)',\n",
              " 'Hoosiers (a.k.a. Best Shot) (1986)']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_movies = [\"Black Panther (2017)\",\n",
        "               \"Avengers, The (2012)\",\n",
        "               \"Avengers: Infinity War - Part I (2018)\",\n",
        "               \"Logan (2017)\",\n",
        "               \"Spider-Man (2002)\",\n",
        "               \"Spider-Man 3 (2007)\"]\n",
        "\n",
        "top_movie = predict(list_movies, model, movie_to_idx, idx_to_movie)\n",
        "top_movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHmsGbxr6JNN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Pirates of the Caribbean: The Curse of the Black Pearl (2003)',\n",
              " 'Evolution (2001)',\n",
              " 'Pulp Fiction (1994)',\n",
              " 'Jupiter Ascending (2015)',\n",
              " 'Kung Fu Panda 3 (2016)',\n",
              " 'Back to the Future (1985)',\n",
              " 'Neighbors (2014)',\n",
              " 'Bridge of Spies (2015)',\n",
              " \"Schindler's List (1993)\",\n",
              " 'Coraline (2009)',\n",
              " 'Spider-Man (2002)',\n",
              " 'Watership Down (1978)',\n",
              " 'X-Men (2000)',\n",
              " 'In the Mood For Love (Fa yeung nin wa) (2000)',\n",
              " 'Burn After Reading (2008)',\n",
              " 'Aladdin (1992)',\n",
              " 'RocknRolla (2008)',\n",
              " \"Before the Devil Knows You're Dead (2007)\",\n",
              " 'Last Castle, The (2001)',\n",
              " 'Looper (2012)',\n",
              " 'Through a Glass Darkly (Såsom i en spegel) (1961)',\n",
              " 'Hoosiers (a.k.a. Best Shot) (1986)',\n",
              " 'Star Wars: Episode III - Revenge of the Sith (2005)',\n",
              " 'Wedding Crashers (2005)',\n",
              " 'Flightplan (2005)',\n",
              " 'Big Fish (2003)',\n",
              " '2046 (2004)',\n",
              " 'Kill Bill: Vol. 2 (2004)',\n",
              " 'Lost in Translation (2003)',\n",
              " 'Clear and Present Danger (1994)']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list_movies = [\"Toy Story 3 (2010)\",\n",
        "               \"Finding Nemo (2003)\",\n",
        "               \"Ratatouille (2007)\",\n",
        "               \"The Lego Movie (2014)\",\n",
        "               \"Ghostbusters (a.k.a. Ghost Busters) (1984)\"]\n",
        "top_movie = predict(list_movies, model, movie_to_idx, idx_to_movie)\n",
        "top_movie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UljgDDKUZIYw"
      },
      "source": [
        "## Unit Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_t_4MbCJ9Oh"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "from numpy.testing import assert_array_equal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezKBo6Q0J-gk"
      },
      "outputs": [],
      "source": [
        "# class TestUtils(unittest.TestCase):\n",
        "#     def testColMapping(self):\n",
        "#         \"test the column mapping function\"\n",
        "#         df = pd.DataFrame(\n",
        "#             {'uid': [1,2,3,4],\n",
        "#              'sid': [1,3,5,7]}\n",
        "#         )\n",
        "#         df, _, _ = map_column(df, col_name='sid')\n",
        "#         assert_array_equal(df.sid_mapped.values,\n",
        "#                            [2, 3, 4, 5])\n",
        "        \n",
        "#     def testSplit(self):\n",
        "#         \"test the train/test/val split\"\n",
        "#         SEED = 42\n",
        "#         df = pd.DataFrame(\n",
        "#             {'uid': list(np.arange(50)),\n",
        "#                 'sid': list(np.arange(50))}\n",
        "#         )\n",
        "#         context = get_context(df, split='train', context_size=5, seed=SEED)\n",
        "#         assert_array_equal(context.sid.values,\n",
        "#                            [12, 13, 14, 15, 16])\n",
        "        \n",
        "#     def testArrayPadding(self):\n",
        "#         \"test array padding function\"\n",
        "#         pad_output_1 = pad_arr(np.array([[1,2,3],[7,8,9]]), expected_size=5)\n",
        "#         pad_output_2 = pad_arr(np.array([[1,2,3]]), expected_size=3)\n",
        "#         assert_array_equal(pad_output_1,\n",
        "#                            [[1, 2, 3],\n",
        "#                             [1, 2, 3],\n",
        "#                             [1, 2, 3],\n",
        "#                             [1, 2, 3],\n",
        "#                             [7, 8, 9]])\n",
        "#         assert_array_equal(pad_output_2,\n",
        "#                            [[1, 2, 3],\n",
        "#                             [1, 2, 3],\n",
        "#                             [1, 2, 3]])\n",
        "        \n",
        "#     def testListPadding(self):\n",
        "#         \"test list padding function\"\n",
        "#         pad_output_1 = pad_list([1,2,3], history_size=5, pad_val=0, mode='left')\n",
        "#         pad_output_2 = pad_list([1,2,3], history_size=6, pad_val=1, mode='right')\n",
        "#         assert_array_equal(pad_output_1,\n",
        "#                            [0, 0, 1, 2, 3])\n",
        "#         assert_array_equal(pad_output_2,\n",
        "#                            [1, 2, 3, 1, 1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TfTrrQ19Q1F"
      },
      "outputs": [],
      "source": [
        "class TestML25Dataset(unittest.TestCase):\n",
        "    def testRecordsCount(self):\n",
        "        train_data = ML25Dataset(args, split='train')\n",
        "        self.assertEqual(len(train_data), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJbEQG5LiXi2"
      },
      "outputs": [],
      "source": [
        "class TestModelBERT4Rec(unittest.TestCase):\n",
        "    def testBERT4Rec(self):\n",
        "        n_items = 1000\n",
        "        recommender = BERT4Rec(args, vocab_size=1000)\n",
        "        src_items = torch.randint(low=0, high=n_items, size=(32, 30))\n",
        "        src_items[:, 0] = 1\n",
        "        trg_out = torch.randint(low=0, high=n_items, size=(32, 30))\n",
        "        out = recommender(src_items)\n",
        "        loss = recommender.training_step((src_items, trg_out), batch_idx=1)\n",
        "        self.assertEqual(out.shape, torch.Size([32, 30, 1000]))\n",
        "        self.assertIsInstance(loss, torch.Tensor)\n",
        "        self.assertFalse(torch.isnan(loss).any())\n",
        "        self.assertEqual(loss.size(),torch.Size([]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVmtbhoeWE3K"
      },
      "outputs": [],
      "source": [
        "class TestModelUtils(unittest.TestCase):\n",
        "    def testMaskedAccuracy(self):\n",
        "        \"test the masked accuracy\"\n",
        "        output1 = masked_accuracy(torch.Tensor([[0,1,1,0]]),\n",
        "                                torch.Tensor([[0,1,1,1]]),\n",
        "                                torch.tensor([1,1,1,1], dtype=torch.bool))\n",
        "\n",
        "        output2 = masked_accuracy(torch.Tensor([[0,1,1,0]]),\n",
        "                                torch.Tensor([[0,1,1,1]]),\n",
        "                                torch.tensor([1,0,0,1], dtype=torch.bool))\n",
        "\n",
        "        self.assertEqual(output1, torch.tensor(0.75, dtype=torch.float64))\n",
        "        self.assertEqual(output2, torch.tensor(0.5, dtype=torch.float64))\n",
        "\n",
        "    def testMaskedCrossEntropy(self):\n",
        "        input = [[1.1049, 1.5729, 1.4864],\n",
        "        [-1.8321, -0.3137, -0.3257]]\n",
        "        target = [0,2]\n",
        "\n",
        "        output1 = masked_ce(torch.tensor(input),\n",
        "                            torch.tensor(target),\n",
        "                            torch.tensor([1,0], dtype=torch.bool))\n",
        "\n",
        "        output2 = masked_ce(torch.tensor(input), \n",
        "                            torch.tensor(target),\n",
        "                            torch.tensor([1,1], dtype=torch.bool))\n",
        "        \n",
        "        assert_array_equal(output1.numpy().round(4),\n",
        "                           np.array(1.4015, dtype=np.float32))\n",
        "        assert_array_equal(output2.numpy().round(4),\n",
        "                           np.array(1.1026, dtype=np.float32))\n",
        "        \n",
        "    def testMaskList(self):\n",
        "        args.SEED = 42\n",
        "        assert_array_equal(mask_list([1,2,3,4,5,6,7,8]),\n",
        "                           [1,2,3,4,5,6,1,8])\n",
        "        args.SEED = 40\n",
        "        assert_array_equal(mask_list([1,2,3,4,5,6,7,8]),\n",
        "                           [1,1,3,4,1,6,7,8])\n",
        "\n",
        "    def testMaskListLastElement(self):\n",
        "        args.SEED = 42\n",
        "        output1 = mask_last_elements_list([1,2,3,4,5,6,7,8], val_context_size=5)\n",
        "        output2 = mask_last_elements_list([1,2,3,4,5,6,7,8], val_context_size=3)\n",
        "        assert_array_equal(output1, [1,2,3,1,5,6,7,1])\n",
        "        assert_array_equal(output2, [1,2,3,4,5,1,7,8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbqpEHhO-94V"
      },
      "outputs": [],
      "source": [
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KlNI72qJZnUv",
        "a2mxEHOOZGWr",
        "ju9vFxseZHCK",
        "V7u3_SyPZHrn",
        "UljgDDKUZIYw",
        "aTzyO6mfZ5cQ",
        "jG04IWFfZ7UT",
        "pQ0_HSBbZ_AL",
        "XcQfTrxmaIFS",
        "FU-1tB3SaK7u",
        "F-RX320PaV6e",
        "v3r-1B3Oaiq7"
      ],
      "name": "T595874_BERT4Rec_on_ML25M_in_PyTorch_Lightning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
